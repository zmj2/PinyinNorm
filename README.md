
# Chinese Text Normalization System

This project is an end-to-end system for normalizing informal, error-prone, or non-standard Chinese input, particularly for cases involving:

- Pinyin input (full pinyin, initials, fuzzy variants)
- Character substitutions (e.g. component combinations like `å¼“è™½` â†’ `å¼º`)
- Typos or phonetic errors (e.g. `doubi` â†’ `é€—æ¯”`)
- Mixed input with emoji, symbols, or non-standard tokens

It is designed as an academic and practical system for analyzing and normalizing natural Chinese user input.

---

## ğŸ§© Key Features

- **Intelligent tokenization**: Custom tokenizer supports pinyin strings, initials, and component combinations, overcoming limitations of standard segmentation.
- **Modular candidate generation**: Combines rules, static dictionaries, component maps, pinyin models, and edit distance.
- **Language model scoring**: Uses a pretrained GPT2 Chinese model (`uer/gpt2-chinese-cluecorpussmall`) to rank sentence candidates by fluency.
- **Customizable pipeline**: Each module is replaceable or extensible â€” e.g. swap language model, dictionary, or scoring logic.

---

## ğŸ“‚ Project Structure

```

project-root/
â”œâ”€â”€ main.py                         # Entry point for normalization
â”œâ”€â”€ normalize.py                   # Custom tokenizer
â”œâ”€â”€ scorer.py                      # GPT-based scoring module
â”œâ”€â”€ candidate\_generator/
â”‚   â”œâ”€â”€ generator.py               # Unified candidate generation
â”‚   â”œâ”€â”€ static\_dict.py             # Static normalization dictionary
â”‚   â”œâ”€â”€ component\_map.py           # Component combination mapping
â”‚   â”œâ”€â”€ edit\_distance.py           # Typo correction by Levenshtein
â”‚   â”œâ”€â”€ fuzzy\_pinyin.py            # Fuzzy pinyin expansion
â”‚   â”œâ”€â”€ pinyin\_input\_method.py     # Full and initial pinyin conversion
â”‚   â””â”€â”€ seq2seq\_model.py           # (Optional) Somiao-style input method
â”œâ”€â”€ dict/
â”‚   â”œâ”€â”€ è§„èŒƒè¯å…¸2000æ¡.txt
â”‚   â””â”€â”€ component\_map.txt
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

---

## ğŸš€ Quick Start

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Copy Pinyin2Hanzi module

> This project depends on the [`Pinyin2Hanzi`](https://github.com/letiantian/Pinyin2Hanzi) library
> Please clone or copy the `Pinyin2Hanzi` folder into the project root.

```bash
git clone https://github.com/letiantian/Pinyin2Hanzi.git
cp -r Pinyin2Hanzi ./Pinyin2Hanzi
```

> Or download manually and ensure `Pinyin2Hanzi/` is in the same folder as `main.py`.

---

### 3. Run normalization

```bash
python main.py "doubiè„¸å¤ªgwiåˆ™äº†"
```

Expected output:

```
ğŸ” Tokens: ['doubi', 'è„¸', 'å¤ª', 'gwiåˆ™', 'äº†']
âœ… Best normalized result: é€—æ¯”è„¸å¤ªè§„åˆ™äº†
ğŸ”¢ Score: -2.83
```

---

## âš™ï¸ Customization

* **Scoring model**: You can replace `GPT2Scorer` with a different language model (e.g. ChatGLM or MacBERT).
* **Dictionaries**: Expand or update `dict/è§„èŒƒè¯å…¸2000æ¡.txt` and `component_map.txt`.
* **Candidate sources**: The generator uses multiple heuristics, and new modules can be easily added.

---

## ğŸ“„ License

MIT License.

---

## ğŸ™ Acknowledgments

* [pkuseg](https://github.com/lancopku/pkuseg-python)
* [pypinyin](https://github.com/mozillazg/python-pinyin)
* [python-Levenshtein](https://github.com/ztane/python-Levenshtein)
* [transformers](https://github.com/huggingface/transformers)
* [UER GPT2 Chinese](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall)

---

## ğŸ‘¤ Author

**Barry Chao**

Undergraduate Student in Artificial Intelligence

Xiamen University, China

* ğŸ“§ Email: [barryjoth@gmail.com](mailto:barryjoth@gmail.com)
* ğŸ§  WeChat: `zmj_418`



