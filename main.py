from normalize import tokenize
from candidate_generator.generator import CandidateGenerator
from scorer import GPT2Scorer

from itertools import product
import sys
import math

def truncate_candidate_lists(candidate_lists, threshold):
    n = len(candidate_lists)
    if n == 0:
        return []

    max_k = int(threshold ** (1 / n))

    truncated = [clist[:max_k] for clist in candidate_lists]
    return truncated

def normalize_sentence(sentence, candidate_generator, scorer, topk=5, max_comb=100):
    tokens = tokenize(sentence)
    print("ğŸ” åˆ†è¯ç»“æœï¼š", tokens)

    candidate_lists = []
    print("\nğŸ¯ å€™é€‰ç»“æœï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ï¼š")
    for token in tokens:
        cands = candidate_generator.get_candidates(token)
        print(f"{token} => {cands}")
        if not cands:
            cands = [token]
        candidate_lists.append(cands)

    total_comb = 1
    for c in candidate_lists:
        total_comb *= len(c)
    if total_comb > max_comb:
        print(f"\nâš ï¸ å€™é€‰ç»„åˆè¿‡å¤šï¼ˆ{total_comb}ï¼‰ï¼Œå·²æˆªæ–­éƒ¨åˆ†å€™é€‰ä»¥æå‡æ€§èƒ½")
        candidate_lists = truncate_candidate_lists(candidate_lists, threshold=max_comb)

    all_sentences = [''.join(words) for words in product(*candidate_lists)]

    scored = [(s, scorer.score(s)) for s in all_sentences]
    scored.sort(key=lambda x: x[1], reverse=True)

    return scored[:topk]

if __name__ == "__main__":
    
    if len(sys.argv) < 2:
        print("ğŸ“¥ ç”¨æ³•: python main.py 'è¾“å…¥å¥å­'")
        sys.exit(0)

    raw_input = sys.argv[1]

    print("ğŸ› ï¸ æ­£åœ¨åŠ è½½å€™é€‰ç”Ÿæˆå™¨å’Œè¯­è¨€æ¨¡å‹...")
    generator = CandidateGenerator()
    scorer = GPT2Scorer()

    result = normalize_sentence(raw_input, generator, scorer)
    print("\nğŸ“Œ ç›¸å¯¹æœ€ä¼˜è¾“å‡ºç»“æœï¼š")
    print(result)

    print("\nâœ… æœ€ä½³è§„èŒƒåŒ–ç»“æœï¼š", result[0][0])
    print("ğŸ”¢ å¾—åˆ†ï¼š", result[0][1])

